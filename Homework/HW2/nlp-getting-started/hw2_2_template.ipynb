{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "s0DBGcZfs0FJ"
      },
      "outputs": [],
      "source": [
        "# e.g. if using google colab import drive, uncomment lines below\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LX0ia6JVtFjr"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "\n",
        "import os\n",
        "import re\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.sparse import hstack\n",
        "from scipy.sparse import vstack\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LinearRegression as sk_OLS\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD0dQabauB7z"
      },
      "source": [
        "# Part (a): Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uRBDrBxYtBbk"
      },
      "outputs": [],
      "source": [
        "#====================================================#\n",
        "# YOUR CODE HERE:\n",
        "#   Import train and test csv files.\n",
        "#   You should use the pd.read_csv function.\n",
        "#   You should set the index_col parameter to equal 'id'.\n",
        "#====================================================#\n",
        "\n",
        "train_data = pd.read_csv('./train.csv', index_col='id')\n",
        "test_data  = pd.read_csv('./test.csv', index_col='id')\n",
        "\n",
        "#====================================================#\n",
        "# END YOUR CODE\n",
        "#====================================================#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dG7kzzuDvlBr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data Shape: (7613,)\n",
            "Test Data Shape: (3263,)\n",
            "Number of labels = 1 in train dataset as percentage: 42.97%\n",
            "Number of labels = 0 in train dataset as percentage: 57.03%\n"
          ]
        }
      ],
      "source": [
        "#====================================================#\n",
        "# YOUR CODE HERE:\n",
        "#   Get the index values for X_train and y_train.\n",
        "#   Get the data values for X_train and y_train.\n",
        "#   Get the index values for X_test.\n",
        "#   Get the index values for y_test.\n",
        "#====================================================#\n",
        "\n",
        "# get train indices\n",
        "X_train_id = train_data.index.to_list()\n",
        "y_train_id = train_data.index.to_list()\n",
        "# get train data\n",
        "X_train    = train_data['text']\n",
        "y_train    = train_data['target']\n",
        "\n",
        "# get test indices\n",
        "X_test_id  = test_data.index.to_list()\n",
        "# get test data\n",
        "X_test     = test_data['text']\n",
        "\n",
        "#====================================================#\n",
        "# END YOUR CODE\n",
        "#====================================================#\n",
        "\n",
        "print(f\"Train Data Shape: {X_train.shape}\")\n",
        "print(f\"Test Data Shape: {X_test.shape}\")\n",
        "\n",
        "print(f\"Number of labels = 1 in train dataset as percentage: {((y_train == 1).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
        "print(f\"Number of labels = 0 in train dataset as percentage: {((y_train == 0).sum() / (X_train.shape[0])) * 100:0.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_16NkNGt3Kx"
      },
      "source": [
        "### Part (a), Question 1: How many training and test data points are there?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uqBPKEYt7W5"
      },
      "source": [
        "### Answer:\n",
        "There are 7613 training data points and 3263 test data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEVclzmqt9TF"
      },
      "source": [
        "### Part (a), Question 2: what percentage of the training tweets are of real disasters, and what percentage is not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J7911HiucTb"
      },
      "source": [
        "### Answer:\n",
        "In the training data, 42.97% of tweets are of real disasters, and 57.03% of tweets are not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrzjUIXfuHEt"
      },
      "source": [
        "# Part (b): Split the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iy51Q0txuJpC"
      },
      "outputs": [],
      "source": [
        "#====================================================#\n",
        "# YOUR CODE HERE:\n",
        "#  You should use the sklearn.model_selection.train_test_split\n",
        "#     parameter to perform the train/development split\n",
        "#   Set the test_size to 0.30.\n",
        "#   Set the random_stat parameter to 42.\n",
        "#====================================================#\n",
        "\n",
        "x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(X_train, y_train, test_size=0.3, random_state= 42)\n",
        "X_train_orig   = x_train_split\n",
        "X_develop_orig = x_test_split\n",
        "y_train_orig   = y_train_split\n",
        "y_develop_orig = y_test_split\n",
        "\n",
        "\n",
        "\n",
        "#====================================================#\n",
        "# END YOUR CODE\n",
        "#====================================================#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSQOZqupuKGO"
      },
      "source": [
        "# Part (c): Preprocess the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wWpMOdJ2uipq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/gareth/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/gareth/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  You should complete the following function to obtain the pre-processed\n",
        "#  X_train and X_develop\n",
        "#  Note that we suggest you to do every sub-question in a dedicated Python\n",
        "#  function to make the code more structured and less error-prone.\n",
        "#  With a function, you can clearly test each part when you encounter an error.\n",
        "#  You can also create your own simple input data (e.g. just one sample) to\n",
        "#  test the correctness of a function.\n",
        "#========================================================================#\n",
        "\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def pre_process(data):\n",
        "    pped_data = pd.DataFrame(data)\n",
        "    \n",
        "    # Remove URL and @\n",
        "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    pped_data['text'] = pped_data['text'].str.replace(url_pattern, '', regex=True)\n",
        "    pped_data['text'] = pped_data['text'].str.replace(r'@\\w+', '', regex=True)\n",
        "    \n",
        "    # Remove Punctuation\n",
        "    pped_data['text'] = pped_data['text'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
        "    # Convert to Lower Case\n",
        "    pped_data['text'] = pped_data['text'].str.lower()\n",
        "    \n",
        "    # Remove Stop Word\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    pped_data['text'] = pped_data['text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "    \n",
        "    return pped_data\n",
        "\n",
        "# Assuming X_train_orig and X_develop_orig are defined somewhere above in your code\n",
        "X_train_preproc   = pre_process(X_train_orig)\n",
        "X_develop_preproc = pre_process(X_develop_orig)\n",
        "\n",
        "\n",
        "# get the preprocessed data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GhiuEjdumEO"
      },
      "source": [
        "# Part (d): Bag of words model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fD4ZzrPQuo7B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5329, 1929)\n",
            "(2284, 1929)\n",
            "(5329,)\n",
            "(2284,)\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  You should complete the following function to obtain X_train and X_develop,\n",
        "#  whose \"text\" feature only contains 1 and 0 to indicate whether a word is in\n",
        "#  the tweet. At this point, you should only be constructing feature vectors\n",
        "#  for each data point using the text in the “text” column.\n",
        "#  You should ignore the “keyword” and “location” columns for now.\n",
        "#========================================================================#\n",
        "\n",
        "def bag_of_word(data, vectorizer=None):\n",
        "    if vectorizer is None:\n",
        "        vectorizer = CountVectorizer(max_features=5000, binary=True, min_df=5)\n",
        "        bow_list = vectorizer.fit_transform(data['text'])\n",
        "    else:\n",
        "        bow_list = vectorizer.transform(data['text'])\n",
        "    return bow_list, vectorizer # Feel free to change the variable name\n",
        "  \n",
        "# get the featurized data\n",
        "X_train, vectorizer_train = bag_of_word(X_train_preproc)\n",
        "X_develop, _ = bag_of_word(X_develop_preproc, vectorizer_train)\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_develop.shape)\n",
        "print(y_train_orig.shape)\n",
        "print(y_develop_orig.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReEOC-rkupXV"
      },
      "source": [
        "# Part (e): Logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vpjvSzoduu3O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 for training set: 0.96\n",
            "F1 for development set: 0.66\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  You should complete the following function for logistic regression\n",
        "#  without regularization terms.\n",
        "#  You will be training logistic regression models using bag of words\n",
        "#  feature vectors obtained in part (d).\n",
        "#========================================================================#\n",
        "\n",
        "def logistic_without_regularization(X_train, Y_train, X_develop, Y_develop):\n",
        "    # initialize your logistic regression model\n",
        "    model = LogisticRegression(penalty=None, solver='saga', max_iter=1000000)\n",
        "\n",
        "    # fit your model to the train data\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    # generate your prediction for the training set\n",
        "    y_train_no_reg = model.predict(X_train)\n",
        "    \n",
        "    # generate your prediction for the development set\n",
        "    y_develop_no_reg = model.predict(X_develop)\n",
        "    \n",
        "    return y_train_no_reg, y_develop_no_reg\n",
        "\n",
        "# get the F1 train and develop scores\n",
        "y_train_no_reg, y_develop_no_reg = logistic_without_regularization(X_train,y_train_orig,X_develop,y_develop_orig)\n",
        "F1_train_no_reg = f1_score(y_train_orig, y_train_no_reg)\n",
        "F1_develop_no_reg = f1_score(y_develop_orig, y_develop_no_reg)\n",
        "\n",
        "print(f\"F1 for training set: {F1_train_no_reg:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_no_reg:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eaZwAJP6KUfv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 for training set: 0.83\n",
            "F1 for development set: 0.73\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  You should complete the following function for logistic regression\n",
        "#  with L1 regularization.\n",
        "#  You will be training logistic regression models using bag of words\n",
        "#  feature vectors obtained in part (d).\n",
        "#========================================================================#\n",
        "\n",
        "def logistic_L1_regularization(X_train, Y_train, X_develop, Y_develop):\n",
        "    # initialize your logistic regression model\n",
        "    model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=10000)\n",
        "    # then fit your model to the train data\n",
        "    model.fit(X_train, Y_train)\n",
        "    # then generate your prediction for the training set\n",
        "    y_train_L1_reg = model.predict(X_train)\n",
        "    # then generate your prediction for the development set\n",
        "    y_develop_L1_reg = model.predict(X_develop)\n",
        "    # then get the weight vector \n",
        "    L1_weight = model.coef_\n",
        "    L1_intercept = model.intercept_\n",
        "\n",
        "    #========================================================================#\n",
        "    #  This function should train a logistic regression model without\n",
        "    #  regularization terms.\n",
        "    #  Report the F1 score in your training and in your development sets.\n",
        "    #========================================================================#\n",
        "    return y_train_L1_reg, y_develop_L1_reg, L1_weight, L1_intercept\n",
        "\n",
        "# get the F1 train and develop scores\n",
        "y_train_L1_reg, y_develop_L1_reg, L1_weight, L1_intercept = logistic_L1_regularization(X_train,y_train_orig,X_develop,y_develop_orig)\n",
        "F1_train_L1_reg = sklearn.metrics.f1_score(y_train_orig, y_train_L1_reg)\n",
        "F1_develop_L1_reg = sklearn.metrics.f1_score(y_develop_orig, y_develop_L1_reg)\n",
        "\n",
        "# print the F1 train and develop scores\n",
        "print(f\"F1 for training set: {F1_train_L1_reg:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_L1_reg:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9PHKy8ElKVAg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 for training set: 0.86\n",
            "F1 for development set: 0.75\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  You should complete the following function for logistic regression\n",
        "#  with L2 regularization.\n",
        "#  You will be training logistic regression models using bag of words\n",
        "#  feature vectors obtained in part (d).\n",
        "#========================================================================#\n",
        "def logistic_L2_regularization(X_train, Y_train, X_develop, Y_develop):\n",
        "    # initialize your logistic regression model\n",
        "    model = LogisticRegression(penalty='l2', solver='saga', max_iter=10000)\n",
        "    # then fit your model to the train data\n",
        "    model.fit(X_train, Y_train)\n",
        "    # then generate your prediction for the training set\n",
        "    y_train_L2_reg = model.predict(X_train)\n",
        "    # then generate your prediction for the development set\n",
        "    y_develop_L2_reg = model.predict(X_develop)\n",
        "    #========================================================================#\n",
        "    #  This function should train a logistic regression model without\n",
        "    #  regularization terms.\n",
        "    #  Report the F1 score in your training and in your development sets.\n",
        "    #========================================================================#\n",
        "    return y_train_L2_reg, y_develop_L2_reg\n",
        "\n",
        "# get the F1 train and develop scores\n",
        "y_train_L2_reg, y_develop_L2_reg = logistic_L2_regularization(X_train,y_train_orig,X_develop,y_develop_orig)\n",
        "F1_train_L2_reg = sklearn.metrics.f1_score(y_train_orig, y_train_L2_reg)\n",
        "F1_develop_L2_reg = sklearn.metrics.f1_score(y_develop_orig, y_develop_L2_reg)\n",
        "\n",
        "# print the F1 train and develop scores\n",
        "\n",
        "print(f\"F1 for training set: {F1_train_L2_reg:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_L2_reg:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DmYQSKkK_L3"
      },
      "source": [
        "### Which one of the three classifiers performed the best on your training and development set? Did you observe any overfitting and did regularization help reduce it? Support your answers with the classifier performance you got."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYpWFNElLbWv"
      },
      "source": [
        "### Answer:\n",
        "\n",
        "In the training data, the None-regularization model performed the best. Its F1 score for training set is 0.97.\n",
        "In the development data, the L2-regularization model performed the best. Its F1 score for development set is 0.75.\n",
        "\n",
        "Based on my view, I think there exists the problem of overfitting in the None-regularization model. Because its F1 score reached 0.96 in the training set but performed poorly in the development set, only 0.66. And with the help of the regularization, the performance in the development set enhanced. In the development set, with L1 regularization, the F1 score reached 0.73 and with the L2 regularization, the F1 score reached 0.75. So, the regularization did help reduce the overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yysent_nLuh1"
      },
      "source": [
        "### Inspect the weight vector of the classifier with L1 regularization (in other words, look at the θ you got after training). You can access the weight vector of the trained model using the coef_attribute of a LogisticRegression instance. What are the most important words for deciding whether a tweet is about a real disaster or not? You might need to run some code (feel free to insert a code cell below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most important word is\n",
            " derailment\n"
          ]
        }
      ],
      "source": [
        "weights = L1_weight[0]\n",
        "index_max = np.argmax(weights)\n",
        "feature_names = vectorizer_train.get_feature_names_out()\n",
        "key_feature = feature_names[index_max]\n",
        "print(\"The most important word is\\n\", key_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ2S1QVCLwuY"
      },
      "source": [
        "### Answer:\n",
        "The most important word for deciding whether a tweet is about a real disaster is **derailment**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UWCsGNruvXD"
      },
      "source": [
        "# Part (f): Bernoulli Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wg7nMMI3u0HY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5329, 1929)\n",
            "F1 for training set: 0.77\n",
            "F1 for development set: 0.72\n"
          ]
        }
      ],
      "source": [
        "class BernoulliNB(object):\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha # alpha to ensure that there will be no zero prior probability\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        #====================================================#\n",
        "        # YOUR CODE HERE:\n",
        "        #  You should build the Bernoully NB model from scratch\n",
        "        #  Do not use sklearn, use numpy and other basic packages\n",
        "        #    only.\n",
        "        #  Please update and save the parameters\n",
        "        #    \"self.class_log_prior_\" and \"self.feature_prob_\"\n",
        "        #  These variables are just a suggestion to help\n",
        "        #    structure your code - you do not need to use them\n",
        "        #    if you would prefer not to\n",
        "        #====================================================#\n",
        "        n_samples, n_features = X.shape\n",
        "        self.classes_ = np.unique(y)\n",
        "        n_classes = len(self.classes_)\n",
        "        self.class_log_prior_ = np.log(np.bincount(y)+ self.alpha) / (n_samples + n_classes * self.alpha)\n",
        "        # log_prior is P(y)\n",
        "        self.feature_prob_ = np.zeros((n_classes, n_features))\n",
        "        for class_ in self.classes_:\n",
        "            rows_to_select = np.where(y == class_)[0]\n",
        "            X_class = X[rows_to_select]\n",
        "            self.feature_prob_[class_, :] = (np.sum(X_class, axis=0) + self.alpha) / (X_class.shape[0] + 2 * self.alpha)\n",
        "\n",
        "            # This is P(x|y)\n",
        "\n",
        "        #====================================================#\n",
        "        # END YOUR CODE\n",
        "        #====================================================#\n",
        "        return self\n",
        "        \n",
        "    \n",
        "    def predict(self, X):\n",
        "        X = X.toarray()\n",
        "        n_samples = X.shape[0]\n",
        "        self.pred_log_prob_ = np.zeros((n_samples, len(self.classes_)))\n",
        "        \n",
        "        for idx, c in enumerate(self.classes_):\n",
        "            # get P(y)\n",
        "            log_prob = np.full(X.shape[0], self.class_log_prior_[c])\n",
        "            # get P(y|x) 在给定特征的情况下，Y类别出现的概率\n",
        "            log_prob += np.log((X * self.feature_prob_[c]).sum(axis=1) + 1e-10)\n",
        "            # 先把y=c的情况下x=1的特征求和 再把x=0的特征求和\n",
        "            log_prob += np.log(((1 - X) * (1 - self.feature_prob_[c])).sum(axis=1))\n",
        "            # 因为有些单词可能只出现在了disaster，但是可能从来没有出现在非disaster之中\n",
        "            self.pred_log_prob_[:, idx] = log_prob\n",
        "        # Search the max value in every row and get the index\n",
        "        y_pred = self.classes_[np.argmax(self.pred_log_prob_, axis=1)]\n",
        "        #====================================================#\n",
        "        # END YOUR CODE\n",
        "        #====================================================#\n",
        "        return y_pred\n",
        "\n",
        "# get the predictions y_train_NB and y_develop_NB\n",
        "nb = BernoulliNB(alpha=1)\n",
        "nb.fit(X_train, y_train_orig)\n",
        "print(X_train.shape)\n",
        "y_train_NB = nb.predict(X_train) # prediction from X_train using model\n",
        "y_develop_NB = nb.predict(X_develop) # prediction from X_develop using model\n",
        "\n",
        "# get the F1 train and develop scores\n",
        "F1_train_NB = sklearn.metrics.f1_score(y_train_orig, y_train_NB)\n",
        "F1_develop_NB = sklearn.metrics.f1_score(y_develop_orig, y_develop_NB)\n",
        "\n",
        "# print the F1 train and develop scores\n",
        "print(f\"F1 for training set: {F1_train_NB:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_NB:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JQrNzE6u0jY"
      },
      "source": [
        "# Part (g): Model comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7BcX1e7Ti0J"
      },
      "source": [
        "Question: Which model performed the best in predicting whether a tweet is of a real disaster or not? Include your performance metric in your response. Comment on the pros and cons of using generative vs discriminative models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7NDJXWFTn4m"
      },
      "source": [
        "Answer: In this project, the logistic regression model performed better. The logistic regression model achieved an F1 score of 0.86 on the training set and 0.75 on the development set, while the Bernoulli naive Bayes model achieved an F1 score of 0.60 on the training set and 0.59 on the development set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTrNTtLgTpFM"
      },
      "source": [
        "Question: hink about the assumptions that Naive Bayes makes. How are the assumptions different from logistic regressions? Discuss whether it is valid and efficient to use Bernoulli Naive Bayes classifier for natural language texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvS8MuqBTqIR"
      },
      "source": [
        "Answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46y89mf2u4q2"
      },
      "source": [
        "# Part (h): N-gram model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def n_gram(data, vectorizer = None):\n",
        "    if vectorizer is None:\n",
        "        vectorizer = CountVectorizer(max_features=5000, binary=True, min_df=5, ngram_range=(2,2))\n",
        "        n_gram = vectorizer.fit_transform(data['text'])\n",
        "    else:\n",
        "        n_gram = vectorizer.transform(data['text'])\n",
        "    return n_gram, vectorizer # Feel free to change the variable name\n",
        "    #========================================================================#\n",
        "    # END CODE HERE\n",
        "    #  This function should return the new data whose \"text\" feature contains\n",
        "    #  only 0 and 1\n",
        "    #========================================================================#\n",
        "\n",
        "# get the featurized data\n",
        "X_train_gram, train_n_gram  = n_gram(X_train_preproc)\n",
        "X_develop_gram, _ = n_gram(X_develop_preproc, train_n_gram)\n",
        "X_train_gram.toarray()\n",
        "X_develop_gram.toarray()\n",
        "matrix_1 = hstack((X_train,X_train_gram))\n",
        "matrix_2 = hstack((X_develop,X_develop_gram))\n",
        "X_train_gram = matrix_1\n",
        "X_develop_gram = matrix_2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "F7q6X9Geu8NB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 for training set: 0.77\n",
            "F1 for development set: 0.73\n",
            "F1 for training set: 0.84\n",
            "F1 for development set: 0.73\n",
            "F1 for training set: 0.87\n",
            "F1 for development set: 0.75\n",
            "F1 for training set: 0.77\n",
            "F1 for development set: 0.73\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  Featurized the preprocessed data: X_train_preproc and X_develop_preproc\n",
        "#  using the N=2 gram model\n",
        "#========================================================================#\n",
        "\n",
        "\n",
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  Use the functions you already defined \"X_train_gram\" and \"X_develop_gram\"\n",
        "#  to re-run:\n",
        "#  Logistic Regression with no regularization Model\n",
        "#  Logistic Regression with L1 regularization Model\n",
        "#  Logistic Regression with L2 regularization Model\n",
        "#========================================================================#\n",
        "y_train_gram_no_reg, y_develop_gram_no_reg = logistic_without_regularization(X_train_gram,y_train_orig,X_develop_gram,y_develop_orig)\n",
        "y_train_gram_L1_reg, y_develop_gram_L1_reg, _, _ = logistic_L1_regularization(X_train_gram,y_train_orig,X_develop_gram,y_develop_orig)\n",
        "y_train_gram_L2_reg, y_develop_gram_L2_reg = logistic_L2_regularization(X_train_gram,y_train_orig,X_develop_gram,y_develop_orig)\n",
        "\n",
        "nb.fit(X_train_gram, y_train_orig)\n",
        "y_train_gram_NB = nb.predict(X_train_gram) # prediction from X_train using model\n",
        "y_develop_gram_NB = nb.predict(X_develop_gram) # prediction from X_develop using model\n",
        "\n",
        "#========================================================================#\n",
        "# END CODE HERE\n",
        "#========================================================================#\n",
        "\n",
        "# get the F1 train and develop scores for no regularization model\n",
        "F1_train_gram_no_reg = sklearn.metrics.f1_score(y_train_orig, y_train_gram_no_reg)\n",
        "F1_develop_gram_no_reg = sklearn.metrics.f1_score(y_develop_orig, y_develop_gram_no_reg)\n",
        "# get the F1 train and develop scores for L1 regularization model\n",
        "F1_train_gram_L1_reg = sklearn.metrics.f1_score(y_train_orig, y_train_gram_L1_reg)\n",
        "F1_develop_gram_L1_reg = sklearn.metrics.f1_score(y_develop_orig, y_develop_gram_L1_reg)\n",
        "# get the F1 train and develop scores for L2 regularization model\n",
        "F1_train_gram_L2_reg = sklearn.metrics.f1_score(y_train_orig, y_train_gram_L2_reg)\n",
        "F1_develop_gram_L2_reg = sklearn.metrics.f1_score(y_develop_orig, y_develop_gram_L2_reg)\n",
        "# get the F1 train and develop scores for Bernoulli NB model\n",
        "F1_train_gram_NB = sklearn.metrics.f1_score(y_train_orig, y_train_gram_NB)\n",
        "F1_develop_gram_NB = sklearn.metrics.f1_score(y_develop_orig, y_develop_gram_NB)\n",
        "\n",
        "# print the F1 train and develop scores for no regularization model\n",
        "print(f\"F1 for training set: {F1_train_gram_NB:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_gram_NB:.2f}\")\n",
        "# print the F1 train and develop scores for L1 regularization model\n",
        "print(f\"F1 for training set: {F1_train_gram_L1_reg:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_gram_L1_reg:.2f}\")\n",
        "# print the F1 train and develop scores for L2 regularization model\n",
        "print(f\"F1 for training set: {F1_train_gram_L2_reg:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_gram_L2_reg:.2f}\")\n",
        "# print the F1 train and develop scores for Bernoulli NB model\n",
        "print(f\"F1 for training set: {F1_train_gram_NB:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_gram_NB:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55QmSb3ku8oa"
      },
      "source": [
        "# Part (i): Determine performance with the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "i270_AqVvAbH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7613, 2580)\n",
            "(7613,)\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  Re-build your feature vectors on the entire Kaggle train set\n",
        "#  (i.e. DO NOT split the train set into a further train set and development set)\n",
        "#========================================================================#\n",
        "X_train = vstack([X_train_gram, X_develop_gram])\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "#========================================================================#\n",
        "# END CODE HERE\n",
        "#========================================================================#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "u985um3AY1ie"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7613, 2544)\n",
            "(7613, 4833)\n",
            "(3263, 4833)\n",
            "(7613,)\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  Re-train your preferred classifier (see below) on the entire train set\n",
        "#  (i.e. DO NOT split the train set into a further train set and development set)\n",
        "#  Your preferred classifier may inculde either bag of word or n-gram,\n",
        "#  and using either logistic regression or Bernoulli naive bayes\n",
        "#========================================================================#\n",
        "X_test = test_data['text']\n",
        "X_final = train_data['text']\n",
        "y_train = train_data['target']\n",
        "X_final_orig = X_final\n",
        "X_test_orig = X_test\n",
        "X_final_preproc = pre_process(X_final_orig)\n",
        "X_test_preproc = pre_process(X_test)\n",
        "\n",
        "def bag_of_word_final(data, vectorizer=None):\n",
        "    if vectorizer is None:\n",
        "        vectorizer = CountVectorizer(max_features=5000, binary=True, min_df=5)\n",
        "        bow_list = vectorizer.fit_transform(data['text'])\n",
        "    else:\n",
        "        bow_list = vectorizer.transform(data['text'])\n",
        "    return bow_list, vectorizer # Feel free to change the variable name\n",
        "  \n",
        "X_final, vectorizer_final = bag_of_word_final(X_final_preproc)\n",
        "print(X_final.shape)\n",
        "X_test, _ = bag_of_word_final(X_test_preproc, vectorizer_final)\n",
        "\n",
        "def n_gram_final(data, vectorizer = None):\n",
        "    if vectorizer is None:\n",
        "        vectorizer = CountVectorizer(max_features=5000, binary=True, min_df=3, ngram_range=(2,2))\n",
        "        n_gram = vectorizer.fit_transform(data['text'])\n",
        "    else:\n",
        "        n_gram = vectorizer.transform(data['text'])\n",
        "    return n_gram, vectorizer\n",
        "\n",
        "X_final_gram, final_n_gram = n_gram_final(X_final_preproc)\n",
        "X_test_gram, _ = n_gram_final(X_test_preproc, final_n_gram)\n",
        "\n",
        "matrix_train = hstack((X_final,X_final_gram))\n",
        "X_final = matrix_train\n",
        "\n",
        "matrix_test = hstack((X_test,X_test_gram))\n",
        "X_test = matrix_test\n",
        "\n",
        "print(X_final.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "\n",
        "def final_model(X_train, Y_train, X_test):\n",
        "    # initialize your logistic regression model\n",
        "    model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=10000)\n",
        "    # then fit your model to the train data\n",
        "    model.fit(X_train, Y_train)\n",
        "    # then generate your prediction for the training set\n",
        "    y_test_L2_reg = model.predict(X_test)\n",
        "    return y_test_L2_reg\n",
        "y_train_pred = final_model(X_final,y_train,X_test)\n",
        "\n",
        "df = pd.DataFrame(y_train_pred)\n",
        "df.to_excel(\"output.xlsx\", index=False, engine='openpyxl')\n",
        "#========================================================================#\n",
        "# END CODE HERE\n",
        "#========================================================================#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final Result is shown as the pic:\n",
        "\n",
        "F-1 Score: 0.79037\n",
        "\n",
        "![My Image](./WeChatc8fe774b7206003cd7939b49665c90c9.jpg)\n",
        "\n",
        "![My Image](./WechatIMG2163.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6CIKyutPY1zL"
      },
      "outputs": [],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  Report the resulting F 1-score on the test data, as reported by Kaggle\n",
        "#========================================================================#\n",
        "\n",
        "#========================================================================#\n",
        "# END CODE HERE\n",
        "#========================================================================#"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
